{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea193c6",
   "metadata": {},
   "source": [
    "Data Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85744af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain.document_loaders import PyMuPDFLoader,PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "976299f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process All PDF Document and convert into Langchain Documents\n",
    "\n",
    "def process_all_pdf(path: str=\"../data/\"):\n",
    "\n",
    "    files = os.listdir(path)\n",
    "    pdf_files = []\n",
    "    documents = []\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdf_files.append(file)\n",
    "\n",
    "    print(f\"No of files {len(pdf_files)}\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "\n",
    "        print(f\"\\nProcessing: {pdf_file}\")\n",
    "        loader = PyMuPDFLoader(path+pdf_file)\n",
    "        document = loader.load()\n",
    "\n",
    "        for doc in document:\n",
    "            doc.metadata[\"FileName\"] = pdf_file\n",
    "            doc.metadata[\"FileType\"] = \"pdf\"\n",
    "\n",
    "        documents.extend(document)    \n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3bc6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = process_all_pdf(\"../data/\")\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc359a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text spiltter and chunk all the documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def splitDocumentToChunks(documents,chunk_size, chunk_overlap):\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\",\"\\n\",\" \",'']\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample Chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "chunks = splitDocumentToChunks(documents,1000,200)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1056fe6",
   "metadata": {},
   "source": [
    "Embeddings And VectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d33a47c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "483ac114",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Embedding Manager class defines methods to load the embedding model and then convert chunks into embeddings (which are vectors)\n",
    "\n",
    "class EmbeddingManager:\n",
    "\n",
    "    def __init__(self,model_name : str = \"all-MiniLM-L6-v2\"):\n",
    "\n",
    "        self.model_name=model_name\n",
    "        self.model=None\n",
    "        self._load_model()\n",
    "\n",
    "    \n",
    "    def _load_model(self):\n",
    "       self.model=SentenceTransformer(self.model_name)\n",
    "\n",
    "    \"\"\"\n",
    "    chunks: list of strings (each chunk's text)\n",
    "    returns: list of embedding vectors (lists of floats)\n",
    "    \"\"\"\n",
    "    def convertChunksToVector(self,chunks: List[str], batch_size: int = 64, normalize: bool=True)->List[List[float]]:\n",
    "        embeddings =  self.model.encode(chunks, \n",
    "                                 batch_size=batch_size,\n",
    "                                 convert_to_numpy=True,\n",
    "                                 normalize_embeddings=normalize,\n",
    "                                 show_progress_bar=True)\n",
    "\n",
    "        return embeddings.tolist()\n",
    "\n",
    "\n",
    "#embedding.convertChunksToVector(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38bdab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ChromaDB manager to upsert embeddings and text,metadatas,ids in the chroma DB\n",
    "\n",
    "class ChromaVectoreStore:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            persist_dir: str=\"../data\",\n",
    "            collection: str = \"rag_corpus\",\n",
    "            space: str=\"cosine\"):\n",
    "        \n",
    "        self.client = chromadb.Client(Settings(persist_directory=persist_dir))\n",
    "\n",
    "        try:\n",
    "            self.col = self.client.get_collection(collection)\n",
    "        except:\n",
    "            self.col=self.client.create_collection(collection,\n",
    "                                                   metadata={\"hnsw:space\":space})    \n",
    "\n",
    "    def insert(self, ids: List[str], documents: List[str], embeddings: List[List[float]], metadatas):\n",
    "\n",
    "        self.col.add(ids=ids,\n",
    "                        documents=documents,\n",
    "                        embeddings=embeddings,\n",
    "                        metadatas=metadatas)\n",
    "\n",
    "\n",
    "\n",
    "    def count(self):\n",
    "        return self.col.count()\n",
    "\n",
    "    def query(self, query_text: str, k: int=5):\n",
    "        return self.col.query(query_texts=[query_text], n_results=5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c67159ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 9/9 [00:02<00:00,  3.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "573"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### convert the chunks page_content from dict into string and upsert/insert in chromaDB\n",
    "\n",
    "docs = [chunk.page_content for chunk in chunks]\n",
    "ids = [str(uuid.uuid4()) for _ in range(len(chunks))]\n",
    "metadetas = [chunk.metadata for chunk in chunks]\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "vectors = embedding_manager.convertChunksToVector(docs)\n",
    "\n",
    "store = ChromaVectoreStore(persist_dir=\"../chroma_store\",collection=\"rag_corpus\")\n",
    "\n",
    "store.insert(ids,docs,vectors,metadetas)\n",
    "store.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "656cfde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval based on Query from vector DB, convert the query to embedding and do search in DB and retrieve\n",
    "\n",
    "class RAGRetriever:\n",
    "\n",
    "    def __init__(self, vector_store: ChromaVectoreStore, embedding_manager: EmbeddingManager):\n",
    "\n",
    "        self.vectoreStore = vector_store\n",
    "        self.embeddingManager = embedding_manager\n",
    "\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int=5, score_threshold: float=0.0) -> List[Dict[str,Any]]:\n",
    "\n",
    "        #Generate Query embedding\n",
    "        queryEmbedding = self.embeddingManager.convertChunksToVector([query])[0]\n",
    "\n",
    "        results = self.vectoreStore.col.query(query_embeddings=queryEmbedding,n_results=top_k)\n",
    "\n",
    "        #Process results\n",
    "        retrieved_docs = []\n",
    "\n",
    "        if results['documents'] and results['distances'][0]:\n",
    "            documents = results['documents'][0]\n",
    "            metadetas = results['metadatas'][0]\n",
    "            distances = results['distances'][0]\n",
    "            ids = results['ids'][0]\n",
    "\n",
    "            for i,(doc_id,document,metadata,distance) in enumerate(zip(ids,documents,metadetas,distances)):\n",
    "\n",
    "                similarity_score = 1-distance \n",
    "\n",
    "                if similarity_score>score_threshold:\n",
    "                    retrieved_docs.append({\n",
    "                        'id': doc_id,\n",
    "                        'content' : document,\n",
    "                        'metadeta' : metadata,\n",
    "                        'similarity_score' : similarity_score,\n",
    "                        'distance' : distance,\n",
    "                        'rank' : i+1\n",
    "                    })\n",
    "\n",
    "                print(f\"Retreived {len(retrieved_docs)} documents (after filtering)\")\n",
    "        else:\n",
    "            print(\"No documents found\")        \n",
    "\n",
    "        return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdfd0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_retriever = RAGRetriever(store,embedding_manager)\n",
    "rag_retriever.retrieve(query=\"What is jdbc Template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c41616",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simplr RAG pipeline with OPENAI LLM\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "def generate_answer(llm : ChatOpenAI, ragRetriever : RAGRetriever, query: str, top_k: int) -> str:\n",
    "\n",
    "    #Retrieve relevant documents\n",
    "    retrieved_docs = ragRetriever.retrieve(query,top_k=top_k)\n",
    "\n",
    "    #Construct context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in retrieved_docs])\n",
    "\n",
    "    #Create prompt\n",
    "    prompt = f\"Use the following context to answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "\n",
    "    #Generate answer using LLM\n",
    "    response = llm.invoke(prompt.format(context=context, query=query))\n",
    "\n",
    "    return response.content\n",
    "\n",
    "answer = generate_answer(llm,rag_retriever,\"Explain JDBC Template in Java\",top_k=3)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7c7d8",
   "metadata": {},
   "source": [
    "Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_advanced(query, rag_retriever, llm, top_k=5,min_score=0.2, return_context=False):\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    retrieved_docs = rag_retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "\n",
    "    print(retrieved_docs)\n",
    "    # Construct context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in retrieved_docs])\n",
    "\n",
    "    sources = [{\n",
    "        'sources': doc['metadeta'].get('source_file',doc['metadeta'].get('source','unknown')),\n",
    "        'page': doc['metadeta'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in retrieved_docs]\n",
    "\n",
    "    confidence = max([doc['similarity_score'] for doc in retrieved_docs])\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"Use the following context to answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "\n",
    "\n",
    "    # Generate answer using LLM\n",
    "    response = llm.invoke(prompt.format(context=context, query=query))\n",
    "\n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'confidence': confidence,\n",
    "        'sources': sources\n",
    "    }\n",
    "\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "    \n",
    "result = rag_advanced(\"What is JDBC Template in Java?\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Context:\", result['context'][:300])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
